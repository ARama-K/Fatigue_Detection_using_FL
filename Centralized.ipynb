{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle/Drive"
      ],
      "metadata": {
        "id": "EISr4m0S2ydx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "bNLpE7z2487f",
        "outputId": "2a579b2d-98a1-485b-ff6f-96c9224dcdbc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2481267056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install kagglehub --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install kagglehub --quiet\n",
        "import kagglehub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and store dataset"
      ],
      "metadata": {
        "id": "mRd5QLqR25tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if not already done)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install Kaggle CLI if missing\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Setup Kaggle API credentials (make sure kaggle.json is in your Drive)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip dataset locally\n",
        "!kaggle datasets download -d tanjemahamed/mental-fatigue-level-detection-fatigueset-data --unzip -p /content/fatigue_data\n",
        "\n",
        "# List local downloaded files to verify\n",
        "!ls /content/fatigue_data\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = '/content/fatigue_data/fatigueset'  # The actual dataset folder\n",
        "drive_dest = '/content/drive/MyDrive/Fatigue_Set'\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(drive_dest, exist_ok=True)\n",
        "\n",
        "# Define full destination path for the folder copy\n",
        "dest_dir = os.path.join(drive_dest, 'fatigueset')\n",
        "\n",
        "# Remove destination folder if it exists to avoid copytree error\n",
        "if os.path.exists(dest_dir):\n",
        "    shutil.rmtree(dest_dir)\n",
        "\n",
        "# Recursively copy entire directory\n",
        "shutil.copytree(source_dir, dest_dir)\n",
        "\n",
        "print(f'Dataset folder copied recursively to: {dest_dir}')"
      ],
      "metadata": {
        "id": "mvMZGaBL5B5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focused files and column structure"
      ],
      "metadata": {
        "id": "ashWAbDg29UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Fatigue_Set/fatigueset'\n",
        "persons = [f'{i:02d}' for i in range(1, 13)]\n",
        "sessions = [f'{i:02d}' for i in range(1, 4)]\n",
        "\n",
        "# Selected sensor files relevant for your fatigue detection model\n",
        "selected_sensor_files = [\n",
        "    'wrist_hr.csv',\n",
        "    'wrist_ibi.csv',\n",
        "    'wrist_acc.csv',\n",
        "    'wrist_eda.csv',\n",
        "    'wrist_skin_temperature.csv',\n",
        "    'exp_fatigue.csv'\n",
        "]\n",
        "\n",
        "for person in persons:\n",
        "    for session in sessions:\n",
        "        session_folder = os.path.join(BASE_PATH, person, session)\n",
        "        print(f'\\nPerson: {person}, Session: {session}')\n",
        "        for sensor_file in selected_sensor_files:\n",
        "            file_path = os.path.join(session_folder, sensor_file)\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, nrows=3)  # Read only first few rows\n",
        "                    print(f'{sensor_file} columns: {list(df.columns)}')\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f'{sensor_file} not found in session {session} of person {person}')\n"
      ],
      "metadata": {
        "id": "buRNFJZd-1nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focused columns sample\n"
      ],
      "metadata": {
        "id": "I3LB-HcG3Del"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Fatigue_Set/fatigueset'\n",
        "PERSONS = [f'{i:02d}' for i in range(1, 13)]\n",
        "SESSIONS = [f'{i:02d}' for i in range(1, 4)]\n",
        "\n",
        "SENSOR_FILES = {\n",
        "    'wrist_hr.csv': ['hr'],\n",
        "    'wrist_ibi.csv': ['duration'],\n",
        "    'wrist_acc.csv': ['ax', 'ay', 'az'],\n",
        "    'wrist_eda.csv': ['eda'],\n",
        "    'wrist_skin_temperature.csv': ['temp']\n",
        "}\n",
        "\n",
        "WINDOW_SIZE_SEC = 30\n",
        "STEP_SIZE_SEC = 15\n",
        "\n",
        "def get_min_sampling_interval(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    if 'timestamp' not in df.columns or df.empty:\n",
        "        return None\n",
        "    ts = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    intervals = ts.diff().dropna()\n",
        "    min_interval = intervals.min()\n",
        "    return min_interval\n",
        "\n",
        "def load_and_resample(filepath, cols, resample_freq):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    original_counts = df[cols].count()\n",
        "\n",
        "    df_resampled = df[cols].resample(resample_freq).mean().interpolate()\n",
        "    resampled_counts = df_resampled.count()\n",
        "\n",
        "    print(f\"File: {os.path.basename(filepath)}\")\n",
        "    for col in cols:\n",
        "        orig = original_counts[col]\n",
        "        resampled = resampled_counts[col]\n",
        "        percent = (resampled / orig * 100) if orig > 0 else 0\n",
        "        print(f\"  Column: {col}, Original entries: {orig}, Resampled entries: {resampled}, Percentage: {percent:.2f}%\")\n",
        "    return df_resampled\n",
        "\n",
        "def load_and_merge_session(person, session):\n",
        "    session_path = os.path.join(BASE_PATH, person, session)\n",
        "    sensor_min_intervals = []\n",
        "\n",
        "    for file_name in SENSOR_FILES.keys():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            min_intv = get_min_sampling_interval(file_path)\n",
        "            if min_intv is not None:\n",
        "                sensor_min_intervals.append(min_intv)\n",
        "    if not sensor_min_intervals:\n",
        "        print(f\"No sensor data found for person {person} session {session}\")\n",
        "        return None\n",
        "\n",
        "    best_interval = max(sensor_min_intervals)\n",
        "    resample_milliseconds = int(best_interval.total_seconds() * 1000)\n",
        "    resample_freq_str = f\"{resample_milliseconds}ms\"\n",
        "    print(f\"\\nPerson {person}, Session {session}, Resampling freq chosen: every {resample_freq_str}\")\n",
        "\n",
        "    data_frames = []\n",
        "    for file_name, cols in SENSOR_FILES.items():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            df_resampled = load_and_resample(file_path, cols, resample_freq_str)\n",
        "            data_frames.append(df_resampled)\n",
        "    if not data_frames:\n",
        "        return None\n",
        "\n",
        "    merged_df = pd.concat(data_frames, axis=1).interpolate().dropna()\n",
        "    print(f\"\\nSynchronized merged data sample for Person {person} Session {session}:\")\n",
        "    print(merged_df.head(20))\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Run for all persons and sessions\n",
        "for person in PERSONS:\n",
        "    for session in SESSIONS:\n",
        "        print(f\"Processing Person {person}, Session {session}\")\n",
        "        merged_data = load_and_merge_session(person, session)\n"
      ],
      "metadata": {
        "id": "4fqgUYpuK4dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features , Labels , Count data"
      ],
      "metadata": {
        "id": "g7iwjqTJ3Lcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Fatigue_Set/fatigueset'\n",
        "PERSONS   = [f'{i:02d}' for i in range(1, 13)]\n",
        "SESSIONS  = [f'{i:02d}' for i in range(1, 4)]\n",
        "\n",
        "SENSOR_FILES = {\n",
        "    'wrist_hr.csv': ['hr'],\n",
        "    'wrist_ibi.csv': ['duration'],\n",
        "    'wrist_acc.csv': ['ax', 'ay', 'az'],\n",
        "    'wrist_eda.csv': ['eda'],\n",
        "    'wrist_skin_temperature.csv': ['temp']\n",
        "}\n",
        "\n",
        "WINDOW_SIZE_SEC = 30\n",
        "STEP_SIZE_SEC   = 15\n",
        "\n",
        "session_type_map = {\n",
        "    '01': 0,  # baseline\n",
        "    '02': 1,  # physical\n",
        "    '03': 2   # mental\n",
        "}\n",
        "\n",
        "# ========== Utilities for Data Processing ==========\n",
        "\n",
        "def get_min_sampling_interval(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    if 'timestamp' not in df.columns or df.empty:\n",
        "        return None\n",
        "    ts = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    intervals = ts.diff().dropna()\n",
        "    return intervals.min()\n",
        "\n",
        "def load_and_resample(filepath, cols, resample_freq):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    df_resampled = df[cols].resample(resample_freq).mean().interpolate()\n",
        "    return df_resampled\n",
        "\n",
        "def load_and_merge_session(person, session):\n",
        "    session_path = os.path.join(BASE_PATH, person, session)\n",
        "    sensor_min_intervals = []\n",
        "    for file_name in SENSOR_FILES.keys():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            min_intv = get_min_sampling_interval(file_path)\n",
        "            if min_intv is not None:\n",
        "                sensor_min_intervals.append(min_intv)\n",
        "    if not sensor_min_intervals:\n",
        "        return None, None, None\n",
        "\n",
        "    best_interval = max(sensor_min_intervals)\n",
        "    resample_milliseconds = int(best_interval.total_seconds() * 1000)\n",
        "    resample_freq_str = f\"{resample_milliseconds}ms\"\n",
        "\n",
        "    data_frames = []\n",
        "    for file_name, cols in SENSOR_FILES.items():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            df_resampled = load_and_resample(file_path, cols, resample_freq_str)\n",
        "            data_frames.append(df_resampled)\n",
        "    if not data_frames:\n",
        "        return None, None, None\n",
        "\n",
        "    merged_df = pd.concat(data_frames, axis=1).interpolate().dropna()\n",
        "    return merged_df, None, None\n",
        "\n",
        "def windowed_segmentation(data, window_size_sec=30, step_size_sec=15, fs_hz=None):\n",
        "    if fs_hz is None:\n",
        "        timedelta = (data.index[1] - data.index[0]).total_seconds()\n",
        "        fs_hz = 1 / timedelta\n",
        "\n",
        "    window_size_samples = int(window_size_sec * fs_hz)\n",
        "    step_size_samples   = int(step_size_sec * fs_hz)\n",
        "\n",
        "    segments, indices = [], []\n",
        "    for start in range(0, len(data) - window_size_samples + 1, step_size_samples):\n",
        "        end = start + window_size_samples\n",
        "        segment = data.iloc[start:end]\n",
        "        segments.append(segment)\n",
        "        indices.append(segment.index[0])\n",
        "    return segments, indices\n",
        "\n",
        "def extract_features(segment):\n",
        "    features = {}\n",
        "    for col in segment.columns:\n",
        "        features[f'{col}_mean'] = segment[col].mean()\n",
        "        features[f'{col}_std']  = segment[col].std()\n",
        "    return features\n",
        "\n",
        "def load_fatigue_labels(person, session, session_start_timestamp):\n",
        "    fatigue_path = os.path.join(BASE_PATH, person, session, 'exp_fatigue.csv')\n",
        "    if not os.path.exists(fatigue_path):\n",
        "        return None\n",
        "    df = pd.read_csv(fatigue_path)\n",
        "    df['fatigueSurveySubmissionDatetime'] = df['fatigueSurveySubmissionTime'].apply(\n",
        "        lambda x: pd.Timestamp(session_start_timestamp) + pd.Timedelta(seconds=x)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def align_labels_to_windows_time_based(label_df, window_starts):\n",
        "    \"\"\"\n",
        "    Instead of nearest label assignment, interpolate fatigue scores over time.\n",
        "    - Assumes label_df has 'fatigueSurveySubmissionDatetime',\n",
        "      'physicalFatigueScore', 'mentalFatigueScore'\n",
        "    - window_starts is a list/array of pandas Timestamps\n",
        "    \"\"\"\n",
        "    submission_times = pd.to_datetime(label_df['fatigueSurveySubmissionDatetime'])\n",
        "    phys_scores = label_df['physicalFatigueScore'].values\n",
        "    ment_scores = label_df['mentalFatigueScore'].values\n",
        "\n",
        "    # Build interpolation functions (time → fatigue score)\n",
        "    phys_interp = np.interp(\n",
        "        [ts.value for ts in window_starts],     # convert to ns int\n",
        "        [t.value for t in submission_times],\n",
        "        phys_scores\n",
        "    )\n",
        "    ment_interp = np.interp(\n",
        "        [ts.value for ts in window_starts],\n",
        "        [t.value for t in submission_times],\n",
        "        ment_scores\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for p, m in zip(phys_interp, ment_interp):\n",
        "        labels.append({\n",
        "            'physicalFatigueScore': p,\n",
        "            'mentalFatigueScore': m\n",
        "        })\n",
        "    return labels\n",
        "\n",
        "\n",
        "def process_person_session(person, session):\n",
        "    merged_df, _, _ = load_and_merge_session(person, session)\n",
        "    if merged_df is None:\n",
        "        return None\n",
        "\n",
        "    ts_deltas = merged_df.index.to_series().diff().dropna()\n",
        "    fs_hz = 1 / ts_deltas.mean().total_seconds()\n",
        "\n",
        "    windows, window_starts = windowed_segmentation(merged_df, WINDOW_SIZE_SEC, STEP_SIZE_SEC, fs_hz)\n",
        "\n",
        "    feature_list = [extract_features(window) for window in windows]\n",
        "    features_df = pd.DataFrame(feature_list)\n",
        "\n",
        "    session_start_timestamp = merged_df.index.min()\n",
        "    fatigue_labels_df = load_fatigue_labels(person, session, session_start_timestamp)\n",
        "    if fatigue_labels_df is None:\n",
        "        return None\n",
        "\n",
        "    labels = align_labels_to_windows_time_based(fatigue_labels_df, window_starts)\n",
        "    labels_df = pd.DataFrame(labels)\n",
        "\n",
        "    merged_features_labels_df = pd.concat([features_df.reset_index(drop=True),\n",
        "                                           labels_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    merged_features_labels_df['window_start'] = pd.to_datetime(window_starts).values\n",
        "    merged_features_labels_df['person'] = person\n",
        "    merged_features_labels_df['session'] = session\n",
        "    return merged_features_labels_df\n",
        "\n",
        "# ========== Build Whole Dataset ==========\n",
        "\n",
        "all_data = []\n",
        "for person in PERSONS:\n",
        "    for session in SESSIONS:\n",
        "        df = process_person_session(person, session)\n",
        "        if df is not None:\n",
        "            all_data.append(df)\n",
        "\n",
        "final_df = pd.concat(all_data, ignore_index=True)\n",
        "print(\"Final dataframe shape:\", final_df.shape)\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_interpolated.csv'\n",
        "final_df.to_csv(save_path, index=False)\n",
        "print(f\"Saved merged dataset to {save_path}\")\n"
      ],
      "metadata": {
        "id": "2cTbWWd8jpqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffld=pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_interpolated.csv')\n",
        "ffld.columns"
      ],
      "metadata": {
        "id": "9GpMBE2m2m6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Normalization"
      ],
      "metadata": {
        "id": "DMepKJDu_yje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_interpolated.csv')\n",
        "\n",
        "# Sensor feature columns\n",
        "feature_cols = [\n",
        "    'hr_mean', 'hr_std', 'duration_mean', 'duration_std',\n",
        "    'ax_mean', 'ax_std', 'ay_mean', 'ay_std', 'az_mean', 'az_std',\n",
        "    'eda_mean', 'eda_std', 'temp_mean', 'temp_std'\n",
        "]\n",
        "\n",
        "# Target label columns\n",
        "label_cols = ['physicalFatigueScore', 'mentalFatigueScore']\n",
        "\n",
        "# ---- Hold-out split BEFORE scaling ----\n",
        "all_pairs = df.groupby(['person','session']).size().index.tolist()\n",
        "random.seed(42); random.shuffle(all_pairs)\n",
        "train_pairs, test_pairs = all_pairs[:31], all_pairs[31:]\n",
        "\n",
        "train_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p,s) in train_pairs])\n",
        "test_df  = pd.concat([df[(df.person==p) & (df.session==s)] for (p,s) in test_pairs])\n",
        "\n",
        "# ---- Fit scaler only on training data ----\n",
        "feat_scaler = MinMaxScaler()\n",
        "train_df[feature_cols] = feat_scaler.fit_transform(train_df[feature_cols])\n",
        "test_df[feature_cols]  = feat_scaler.transform(test_df[feature_cols])\n",
        "joblib.dump(feat_scaler, 'feature_scaler.save')\n",
        "\n",
        "# ---- Scale labels (optional: usually for regression stability) ----\n",
        "label_scaler = MinMaxScaler()\n",
        "train_df[label_cols] = label_scaler.fit_transform(train_df[label_cols])\n",
        "test_df[label_cols]  = label_scaler.transform(test_df[label_cols])\n",
        "joblib.dump(label_scaler, 'label_scaler.save')\n",
        "\n",
        "# ---- Save normalized train/test sets ----\n",
        "train_df.to_csv('/content/drive/MyDrive/Fatigue_Set/final_train_normalized.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/Fatigue_Set/final_test_normalized.csv', index=False)\n",
        "\n",
        "print(\"Features + Labels normalized (train/test separately) and saved.\")"
      ],
      "metadata": {
        "id": "BKHsgFUW5JgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_train_normalized.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CFr5fxlo329i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "\n",
        "# Count entries per (person, session)\n",
        "counts = df.groupby(['person', 'session']).size().reset_index(name='count')\n",
        "\n",
        "print(counts)\n",
        "\n",
        "total_counts = df.groupby('person').size().reset_index(name='total_count')\n",
        "\n",
        "print(total_counts)"
      ],
      "metadata": {
        "id": "GI-9aamGAzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single Client - train with 3 X 3 sessions , test with 1 X 1 session (Desired model is after time sequencing model)"
      ],
      "metadata": {
        "id": "jfqPxVtJ_1lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base line model (without time sequencing)"
      ],
      "metadata": {
        "id": "kFb81XEhyym9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "\n",
        "# ========== Model Components ==========\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, n_modalities, hidden_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        concat_dim = n_modalities * hidden_dim\n",
        "        self.query = nn.Linear(concat_dim, fusion_dim)\n",
        "        self.key   = nn.Linear(concat_dim, fusion_dim)\n",
        "        self.value = nn.Linear(concat_dim, fusion_dim)\n",
        "    def forward(self, features, domain_disc):\n",
        "        x = torch.cat(features, dim=1)\n",
        "        Q, K, V = self.query(x), self.key(x), self.value(x)\n",
        "        attn_scores = torch.matmul(Q, K.T) / (K.size(-1) ** 0.5)\n",
        "        attn_scores = attn_scores + domain_disc\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        return out\n",
        "\n",
        "class GradReverse(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambd):\n",
        "        ctx.lambd = lambd\n",
        "        return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.lambd, None\n",
        "\n",
        "class GradientReversal(nn.Module):\n",
        "    def __init__(self, lambd=1.0):\n",
        "        super().__init__()\n",
        "        self.lambd = lambd\n",
        "    def forward(self, x):\n",
        "        return GradReverse.apply(x, self.lambd)\n",
        "\n",
        "class DomainAdaptiveLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.grl = GradientReversal()\n",
        "        self.fc = nn.Linear(input_dim, input_dim)\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.grl(x))\n",
        "\n",
        "class FMAL_Daf(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(inp_dim, lstm_hidden) for inp_dim in modalities_dim])\n",
        "        self.attn_fusion = CrossModalAttention(len(modalities_dim), lstm_hidden, fusion_dim)\n",
        "        self.domain_adapt = DomainAdaptiveLayer(fusion_dim)\n",
        "        self.global_lstm = nn.LSTM(fusion_dim, 32, batch_first=True)\n",
        "        self.reg_head_phys = nn.Linear(32, 1)\n",
        "        self.reg_head_ment = nn.Linear(32, 1)\n",
        "        self.class_head = nn.Linear(32, 3)\n",
        "    def forward(self, modal_inputs, domain_disc):\n",
        "        feats = [mod(mod_inp) for mod, mod_inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = self.attn_fusion(feats, domain_disc)\n",
        "        adapted = self.domain_adapt(fused)\n",
        "        glstm_out, _ = self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_head_phys(feat), self.reg_head_ment(feat), self.class_head(feat)\n",
        "\n",
        "# ========== Dataset Loader ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df.reset_index(drop=True)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'], row['ax_std'], row['ay_mean'], row['ay_std'], row['az_mean'], row['az_std']], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "        phys = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "        session_type = torch.tensor(0, dtype=torch.long)  # placeholder\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)  # placeholder\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, (phys, ment, session_type)\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([sample[0][i] for sample in batch]) for i in range(5)]\n",
        "    domain_disc = torch.stack([sample[1] for sample in batch])\n",
        "    phys = torch.stack([sample[2][0] for sample in batch])\n",
        "    ment = torch.stack([sample[2][1] for sample in batch])\n",
        "    stype = torch.stack([sample[2][2] for sample in batch])\n",
        "    return modal_inputs, domain_disc, (phys, ment, stype)\n",
        "\n",
        "# ========== Data Preparation ==========\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "\n",
        "all_pairs = df.groupby(['person', 'session']).size().index.tolist()\n",
        "\n",
        "# One client: 3 persons × 3 sessions = 9 pairs\n",
        "train_pairs = all_pairs[:9]\n",
        "# One test pair\n",
        "test_pair = all_pairs[9]\n",
        "\n",
        "# ✅ Print which person–session pairs are chosen\n",
        "print(\"Training person-session pairs:\")\n",
        "for p, s in train_pairs:\n",
        "    print(f\"  Person {p}, Session {s}\")\n",
        "\n",
        "print(\"\\nTesting person-session pair:\")\n",
        "print(f\"  Person {test_pair[0]}, Session {test_pair[1]}\")\n",
        "\n",
        "# Build client dataset (only 1 client here)\n",
        "client_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p, s) in train_pairs]).drop(columns=['person'])\n",
        "clients_datasets = [FatigueSessionDataset(client_df)]\n",
        "\n",
        "# Test dataset\n",
        "test_df = df[(df.person == test_pair[0]) & (df.session == test_pair[1])]\n",
        "test_df= test_df.drop(columns=['person'])\n",
        "test_dataset = FatigueSessionDataset(test_df)\n",
        "\n",
        "# Loaders\n",
        "clients_loaders = [DataLoader(clients_datasets[0], batch_size=32, shuffle=True, collate_fn=collate)]\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# ========== Training ==========\n",
        "modal_dims = [2, 2, 6, 2, 2]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "global_model = FMAL_Daf(modal_dims).to(device)\n",
        "\n",
        "criterion_reg = nn.MSELoss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_client(model, loader, client_id, rnd):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    total_loss = 0\n",
        "    for batch_idx, (modal_inputs, domain_disc, (phys, ment, stype)) in enumerate(loader):\n",
        "        modal_inputs = [x.to(device) for x in modal_inputs]\n",
        "        domain_disc = domain_disc.to(device)\n",
        "        phys, ment, stype = phys.to(device), ment.to(device), stype.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        phys_pred, ment_pred, stype_pred = model(modal_inputs, domain_disc)\n",
        "        loss = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "        loss += criterion_cls(stype_pred, stype)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"[Round {rnd+1}] Client {client_id+1} Avg Training Loss: {avg_loss:.4f}\")\n",
        "    return model.state_dict()\n",
        "\n",
        "optimizer = torch.optim.Adam(global_model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    global_model.train()\n",
        "    total_loss = 0\n",
        "    for modal_inputs, domain_disc, (phys, ment, stype) in clients_loaders[0]:\n",
        "        modal_inputs = [x.to(device) for x in modal_inputs]\n",
        "        domain_disc = domain_disc.to(device)\n",
        "        phys, ment, stype = phys.to(device), ment.to(device), stype.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        phys_pred, ment_pred, stype_pred = global_model(modal_inputs, domain_disc)\n",
        "        loss = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "        loss += criterion_cls(stype_pred, stype)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Avg Training Loss: {total_loss / len(clients_loaders[0]):.4f}\")\n",
        "\n",
        "\n",
        "# ========== Testing ==========\n",
        "global_model.eval()\n",
        "reg_losses, cls_losses = [], []\n",
        "with torch.no_grad():\n",
        "    for modal_inputs, domain_disc, (phys, ment, stype) in test_loader:\n",
        "        modal_inputs = [x.to(device) for x in modal_inputs]\n",
        "        domain_disc = domain_disc.to(device)\n",
        "        phys, ment, stype = phys.to(device), ment.to(device), stype.to(device)\n",
        "        phys_pred, ment_pred, stype_pred = global_model(modal_inputs, domain_disc)\n",
        "        reg_loss = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "        cls_loss = criterion_cls(stype_pred, stype)\n",
        "        reg_losses.append(reg_loss.item())\n",
        "        cls_losses.append(cls_loss.item())\n",
        "\n",
        "print(\"\\n=== Final Test Performance ===\")\n",
        "print(f\"Test Regression Loss: {np.mean(reg_losses):.4f}\")\n",
        "print(f\"Test Classification Loss: {np.mean(cls_losses):.4f}\")\n"
      ],
      "metadata": {
        "id": "FnhQ1sHl7Kx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREVIOUSLY WITHOUT TIME SEQUENCING"
      ],
      "metadata": {
        "id": "qtClHndtyivz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ========== Dataset Loader ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df.reset_index(drop=True)\n",
        "        self.session_map = {1: 0, 2: 1, 3: 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # --- Features ---\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'], row['ax_std'],\n",
        "                            row['ay_mean'], row['ay_std'],\n",
        "                            row['az_mean'], row['az_std']], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        # --- Labels ---\n",
        "        phys = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "\n",
        "        session_type = torch.tensor(self.session_map[row['session']], dtype=torch.long)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)  # placeholder\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, (phys, ment, session_type)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([sample[0][i] for sample in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([sample[1] for sample in batch])\n",
        "    phys         = torch.stack([sample[2][0] for sample in batch])\n",
        "    ment         = torch.stack([sample[2][1] for sample in batch])\n",
        "    stype        = torch.stack([sample[2][2] for sample in batch])\n",
        "    return modal_inputs, domain_disc, (phys, ment, stype)\n",
        "\n",
        "\n",
        "# ========== Model Architecture ==========\n",
        "\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, n_modalities, hidden_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        # Instead of concatenation, we expect [batch, n_modalities, hidden_dim]\n",
        "        self.query = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.key   = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.value = nn.Linear(hidden_dim, fusion_dim)\n",
        "\n",
        "    def forward(self, features, domain_disc):\n",
        "        # features: list of modality outputs, each [batch, hidden_dim]\n",
        "        # Stack to [batch, n_modalities, hidden_dim]\n",
        "        x = torch.stack(features, dim=1)\n",
        "\n",
        "        Q = self.query(x)  # [batch, n_modalities, fusion_dim]\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Compute attention scores along modality dim\n",
        "        # attn_scores shape: [batch, n_modalities, n_modalities]\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "\n",
        "        # domain_disc shape is [batch, 1] or [batch, dim], broadcasting accordingly\n",
        "        # Add domain distribution bias if provided\n",
        "        if domain_disc is not None and domain_disc.numel() > 1:\n",
        "            # reshape if needed\n",
        "            domain_disc = domain_disc.unsqueeze(1).expand(-1, attn_scores.size(1), attn_scores.size(2))\n",
        "            attn_scores = attn_scores + domain_disc\n",
        "        else:\n",
        "            # domain_disc is placeholder scalar, ignore\n",
        "            pass\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # weights over modalities\n",
        "        out = torch.matmul(attn_weights, V)  # [batch, n_modalities, fusion_dim]\n",
        "\n",
        "        # Aggregate modalities by averaging weighted outputs per sample\n",
        "        out = out.mean(dim=1)  # [batch, fusion_dim]\n",
        "        return out\n",
        "\n",
        "\n",
        "class FMAL_Daf_Modified(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=32, fusion_dim=64, use_grl=False):\n",
        "        super().__init__()\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(inp_dim, lstm_hidden) for inp_dim in modalities_dim])\n",
        "        self.attn_fusion = CrossModalAttention(len(modalities_dim), lstm_hidden, fusion_dim)\n",
        "        self.attn_dropout = nn.Dropout(0.2)  # Added dropout\n",
        "        # Domain adaptation disabled for debugging\n",
        "        self.domain_adapt = nn.Identity() if not use_grl else nn.Linear(fusion_dim, fusion_dim)\n",
        "        self.global_lstm = nn.LSTM(fusion_dim, 32, batch_first=True)\n",
        "        self.reg_head_phys = nn.Linear(32, 1)\n",
        "        self.reg_head_ment = nn.Linear(32, 1)\n",
        "        self.class_head = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, modal_inputs, domain_disc):\n",
        "        feats = [mod(mod_inp) for mod, mod_inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = self.attn_fusion(feats, domain_disc)\n",
        "        fused = self.attn_dropout(fused)\n",
        "        adapted = self.domain_adapt(fused)\n",
        "        glstm_out, _ = self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_head_phys(feat), self.reg_head_ment(feat), self.class_head(feat)\n",
        "\n",
        "\n",
        "# ========== Training & Evaluation Functions ==========\n",
        "def train_model(model, loader, optimizer, criterion_reg, criterion_cls, epochs=10):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for modal_inputs, domain_disc, (phys, ment, stype) in loader:\n",
        "            modal_inputs = [x for x in modal_inputs]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move domain_disc to device if not None\n",
        "            domain_disc = domain_disc.to(modal_inputs[0].device)\n",
        "\n",
        "            phys_pred, ment_pred, cls_pred = model(modal_inputs, domain_disc)\n",
        "\n",
        "            loss_reg = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "            loss_cls = criterion_cls(cls_pred, stype)\n",
        "            loss = loss_reg + 0.1 * loss_cls  # Scale classification loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"[Epoch {epoch}] Avg Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model, test_loader, device, criterion_reg, criterion_cls):\n",
        "    model.eval()\n",
        "    total_reg_loss, total_cls_loss = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            features, domain_disc, (true_phys, true_ment, true_stype) = batch\n",
        "            features = [x.to(device) for x in features]\n",
        "            true_phys = true_phys.to(device)\n",
        "            true_ment = true_ment.to(device)\n",
        "            true_stype = true_stype.to(device)\n",
        "\n",
        "            domain_disc = domain_disc.to(device)\n",
        "\n",
        "            pred_phys, pred_ment, pred_cls = model(features, domain_disc)\n",
        "            loss_reg = criterion_reg(pred_phys, true_phys) + criterion_reg(pred_ment, true_ment)\n",
        "            loss_cls = criterion_cls(pred_cls, true_stype)\n",
        "\n",
        "            total_reg_loss += loss_reg.item()\n",
        "            total_cls_loss += loss_cls.item()\n",
        "\n",
        "            pred_cls_labels = torch.argmax(pred_cls, dim=1)\n",
        "\n",
        "            print(\"=== Predictions vs Ground Truth ===\")\n",
        "            for i in range(len(pred_phys)):\n",
        "                print(f\"Sample {i}:\")\n",
        "                print(f\"  True phys={true_phys[i].item():.3f}, True ment={true_ment[i].item():.3f}, \"\n",
        "                      f\"True session_type={true_stype[i].item()}\")\n",
        "                print(f\"  Pred phys={pred_phys[i].item():.3f}, Pred ment={pred_ment[i].item():.3f}, \"\n",
        "                      f\"Pred session_type={pred_cls_labels[i].item()}\")\n",
        "            break  # only first batch\n",
        "\n",
        "    avg_reg_loss = total_reg_loss / len(test_loader)\n",
        "    avg_cls_loss = total_cls_loss / len(test_loader)\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss: {avg_reg_loss:.4f}\")\n",
        "    print(f\"Test Classification Loss: {avg_cls_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ========== Main ==========\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "\n",
        "# Extract all unique (person, session) pairs\n",
        "all_pairs = df.groupby(['person', 'session']).size().index.tolist()\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "# Select 9 pairs for training and 3 for testing\n",
        "train_pairs = all_pairs[:9]\n",
        "test_pairs = all_pairs[9:12]  # next 3 pairs\n",
        "\n",
        "print(\"Training person-session pairs:\")\n",
        "for (p, s) in train_pairs:\n",
        "    print(f\"  Person {int(p)}, Session {int(s)}\")\n",
        "\n",
        "print(\"\\nTesting person-session pairs:\")\n",
        "for (p, s) in test_pairs:\n",
        "    print(f\"  Person {int(p)}, Session {int(s)}\")\n",
        "\n",
        "# Concatenate data for train and test sets respectively\n",
        "train_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p, s) in train_pairs]).drop(columns=['person'])\n",
        "test_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p, s) in test_pairs]).drop(columns=['person'])\n",
        "\n",
        "train_dataset = FatigueSessionDataset(train_df)\n",
        "test_dataset = FatigueSessionDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "print(\"\\n=== Sample from Training Data ===\")\n",
        "for i in range(3):\n",
        "    features, _, (phys, ment, stype) = train_dataset[i]\n",
        "    print(f\"Sample {i}: phys={phys.item():.3f}, ment={ment.item():.3f}, session_type={stype.item()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "modal_dims = [2, 2, 6, 2, 2]\n",
        "model = FMAL_Daf_Modified(modal_dims, use_grl=False).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion_reg = nn.MSELoss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion_reg, criterion_cls, epochs=10)\n",
        "evaluate_and_show_predictions(model, test_loader, device, criterion_reg, criterion_cls)\n"
      ],
      "metadata": {
        "id": "RjIQCqR7ZdH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AFTER TIME SEQUENCING (SESSIONS-WISE SEQUENCE-WISE)"
      ],
      "metadata": {
        "id": "DeZiCeLXynU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd, numpy as np, random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "# ========== Dataset ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Groups rows into sessions. Each __getitem__ = one timestep from one session.\n",
        "    Why? -> To keep sample-level printing like your original \"Sample 0: phys=..\"\n",
        "    \"\"\"\n",
        "    def __init__(self, df, session_map=None):\n",
        "        self.data = df.sort_values([\"session\",\"window_start\"]).reset_index(drop=True)\n",
        "        self.session_map = session_map if session_map else {1:0,2:1,3:2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'],row['ax_std'],row['ay_mean'],row['ay_std'],row['az_mean'],row['az_std']],dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        phys = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "        stype = torch.tensor(self.session_map[row['session']], dtype=torch.long)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)\n",
        "        return [HR,IBI,ACC,EDA,Temp], domain_disc, (phys,ment,stype)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([b[0][i] for b in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([b[1] for b in batch])\n",
        "    phys         = torch.stack([b[2][0] for b in batch])\n",
        "    ment         = torch.stack([b[2][1] for b in batch])\n",
        "    stype        = torch.stack([b[2][2] for b in batch])\n",
        "    return modal_inputs, domain_disc, (phys,ment,stype)\n",
        "\n",
        "\n",
        "# ========== Model ==========\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim,hidden_dim,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        if x.ndim==2:\n",
        "            x = x.unsqueeze(1)      # single-step fallback\n",
        "        out,_ = self.lstm(x)\n",
        "        return out[:,-1,:]\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self,n_modalities,hidden_dim=32,fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim,fusion_dim)\n",
        "        self.key   = nn.Linear(hidden_dim,fusion_dim)\n",
        "        self.value = nn.Linear(hidden_dim,fusion_dim)\n",
        "    def forward(self,features,domain_disc):\n",
        "        x = torch.stack(features,1)          # [B,M,H]\n",
        "        Q,K,V = self.query(x),self.key(x),self.value(x)\n",
        "        scores = torch.matmul(Q,K.transpose(-2,-1))/(K.size(-1)**0.5)\n",
        "        attn = F.softmax(scores,dim=-1)\n",
        "        out = torch.matmul(attn,V).mean(1)   # [B,Fusion]\n",
        "        return out\n",
        "\n",
        "\n",
        "class DomainAdaptiveLayer(nn.Module):\n",
        "    def __init__(self,dim): super().__init__(); self.fc=nn.Linear(dim,dim)\n",
        "    def forward(self,x): return self.fc(x)\n",
        "\n",
        "\n",
        "class FMAL_Daf(nn.Module):\n",
        "    def __init__(self,modalities_dim,lstm_hidden=32,fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.modality_lstms=nn.ModuleList([ModalityLSTM(d,lstm_hidden) for d in modalities_dim])\n",
        "        self.attn_fusion=CrossModalAttention(len(modalities_dim),lstm_hidden,fusion_dim)\n",
        "        self.domain_adapt=DomainAdaptiveLayer(fusion_dim)\n",
        "        self.global_lstm=nn.LSTM(fusion_dim,32,batch_first=True)\n",
        "        self.reg_phys=nn.Linear(32,1)\n",
        "        self.reg_ment=nn.Linear(32,1)\n",
        "        self.class_head=nn.Linear(32,3)\n",
        "    def forward(self,modal_inputs,domain_disc):\n",
        "        feats=[m(inp) for m,inp in zip(self.modality_lstms,modal_inputs)]\n",
        "        fused=self.attn_fusion(feats,domain_disc)\n",
        "        adapted=self.domain_adapt(fused)\n",
        "        glstm_out,_=self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat=glstm_out[:,-1,:]\n",
        "        return self.reg_phys(feat),self.reg_ment(feat),self.class_head(feat)\n",
        "\n",
        "\n",
        "# ========== Train / Eval ==========\n",
        "def train_model(model,loader,optim,crit_reg,crit_cls,epochs=10):\n",
        "    for ep in range(1,epochs+1):\n",
        "        model.train(); tot=0\n",
        "        for modals,dom,(phys,ment,stype) in loader:\n",
        "            modals=[m.to(device) for m in modals]\n",
        "            dom=dom.to(device); phys,ment,stype=phys.to(device),ment.to(device),stype.to(device)\n",
        "            optim.zero_grad()\n",
        "            p,m,s=model(modals,dom)\n",
        "            loss=crit_reg(p,phys)+crit_reg(m,ment)+0.1*crit_cls(s,stype)\n",
        "            loss.backward(); optim.step(); tot+=loss.item()\n",
        "        print(f\"[Epoch {ep}] Avg Training Loss: {tot/len(loader):.4f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model,test_loader,device,crit_reg,crit_cls):\n",
        "    model.eval(); regL,clsL=0,0\n",
        "    with torch.no_grad():\n",
        "        for modals,dom,(phys,ment,stype) in test_loader:\n",
        "            modals=[m.to(device) for m in modals]\n",
        "            phys,ment,stype=phys.to(device),ment.to(device),stype.to(device)\n",
        "            dom=dom.to(device)\n",
        "            p_pred,m_pred,s_pred=model(modals,dom)\n",
        "            loss_r=crit_reg(p_pred,phys)+crit_reg(m_pred,ment)\n",
        "            loss_c=crit_cls(s_pred,stype)\n",
        "            regL+=loss_r.item(); clsL+=loss_c.item()\n",
        "            pred_cls=torch.argmax(s_pred,1)\n",
        "            print(\"=== Predictions vs Ground Truth ===\")\n",
        "            for i in range(len(phys)):\n",
        "                print(f\"Sample {i}:\")\n",
        "                print(f\"  True phys={phys[i].item():.3f}, True ment={ment[i].item():.3f}, True session_type={stype[i].item()}\")\n",
        "                print(f\"  Pred phys={p_pred[i].item():.3f}, Pred ment={m_pred[i].item():.3f}, Pred session_type={pred_cls[i].item()}\")\n",
        "            break\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss: {regL/len(test_loader):.4f}\")\n",
        "    print(f\"Test Classification Loss: {clsL/len(test_loader):.4f}\")\n",
        "\n",
        "\n",
        "# ========== Main ==========\n",
        "df=pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "all_pairs=df.groupby(['person','session']).size().index.tolist()\n",
        "random.seed(42); random.shuffle(all_pairs)\n",
        "train_pairs,test_pairs=all_pairs[:9],all_pairs[9:12]\n",
        "\n",
        "print(\"Training person-session pairs:\")\n",
        "for p,s in train_pairs: print(f\"  Person {p}, Session {s}\")\n",
        "print(\"\\nTesting person-session pairs:\")\n",
        "for p,s in test_pairs: print(f\"  Person {p}, Session {s}\")\n",
        "\n",
        "train_df=pd.concat([df[(df.person==p)&(df.session==s)] for (p,s) in train_pairs])\n",
        "test_df=pd.concat([df[(df.person==p)&(df.session==s)] for (p,s) in test_pairs])\n",
        "\n",
        "train_dataset=FatigueSessionDataset(train_df)\n",
        "test_dataset=FatigueSessionDataset(test_df)\n",
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=collate)\n",
        "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=False,collate_fn=collate)\n",
        "\n",
        "print(\"\\n=== Sample from Training Data ===\")\n",
        "for i in range(3):\n",
        "    feats,_,(phys,ment,stype)=train_dataset[i]\n",
        "    print(f\"Sample {i}: phys={phys.item():.3f}, ment={ment.item():.3f}, session_type={stype.item()}\")\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=FMAL_Daf([2,2,6,2,2]).to(device)\n",
        "optim=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "crit_reg,crit_cls=nn.MSELoss(),nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model,train_loader,optim,crit_reg,crit_cls,epochs=10)\n",
        "evaluate_and_show_predictions(model,test_loader,device,crit_reg,crit_cls)\n"
      ],
      "metadata": {
        "id": "LO7t2hkfx6v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Centralized Model - With Novelty\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j7dTyuwYbTub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# =========================\n",
        "# Config / Debug\n",
        "# =========================\n",
        "DEBUG_ATTENTION = False  # set True to print domain scaling + attention weights for the first batch\n",
        "\n",
        "# ========== Dataset ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each __getitem__ = one timestep.\n",
        "    - Converts session to int and uses it as a numeric feature (optionally can embed later).\n",
        "    - Computes elapsed_time within each (person, session) from window_start.\n",
        "    - Returns modal tensors, domain_disc placeholder, session_feat, elapsed_time, and targets.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, session_map=None, compute_elapsed=True):\n",
        "        df = df.copy()\n",
        "        # ensure correct types\n",
        "        df['session'] = df['session'].astype(int)\n",
        "        df['person'] = df['person'].astype(int)\n",
        "        # parse window_start as datetime\n",
        "        df['window_start'] = pd.to_datetime(df['window_start'])\n",
        "\n",
        "        # compute elapsed time (seconds) within each person-session\n",
        "        if compute_elapsed:\n",
        "            df['elapsed_time'] = df.groupby(['person', 'session'])['window_start'] \\\n",
        "                                  .transform(lambda x: (x - x.min()).dt.total_seconds())\n",
        "        else:\n",
        "            df['elapsed_time'] = 0.0\n",
        "\n",
        "        # sort to preserve temporal order per session\n",
        "        self.data = df.sort_values(['person', 'session', 'window_start']).reset_index(drop=True)\n",
        "\n",
        "        # Build session_map if not given (keeps compatibility with older code, but not strictly needed)\n",
        "        if session_map is None:\n",
        "            all_sessions = sorted(self.data['session'].unique())\n",
        "            self.session_map = {s: i for i, s in enumerate(all_sessions)}\n",
        "            # print mapping for debugging\n",
        "            print(\"Session mapping:\", self.session_map)\n",
        "        else:\n",
        "            self.session_map = session_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Modalities (same as your original layout)\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([\n",
        "            row['ax_mean'], row['ax_std'],\n",
        "            row['ay_mean'], row['ay_std'],\n",
        "            row['az_mean'], row['az_std']\n",
        "        ], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        # Targets (regression)\n",
        "        phys  = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment  = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "\n",
        "        # Session numeric feature (use session_map to keep small integers if desired)\n",
        "        session_index = self.session_map[int(row['session'])]\n",
        "        session_feat = torch.tensor([float(session_index)], dtype=torch.float32)\n",
        "\n",
        "        # Elapsed time feature (seconds)\n",
        "        elapsed = torch.tensor([float(row['elapsed_time'])], dtype=torch.float32)\n",
        "\n",
        "        # Domain discrepancy placeholder (unused for now but kept to preserve architecture)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)\n",
        "\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, session_feat, elapsed, (phys, ment)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    # Stack modality tensors: output is a list of 5 tensors each [B, feature_dim]\n",
        "    modal_inputs = [torch.stack([b[0][i] for b in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([b[1] for b in batch])\n",
        "    session_feat = torch.stack([b[2] for b in batch])    # [B, 1]\n",
        "    elapsed_feat = torch.stack([b[3] for b in batch])    # [B, 1]\n",
        "    phys         = torch.stack([b[4][0] for b in batch])\n",
        "    ment         = torch.stack([b[4][1] for b in batch])\n",
        "    return modal_inputs, domain_disc, session_feat, elapsed_feat, (phys, ment)\n",
        "\n",
        "\n",
        "# ========== Model ==========\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # small LSTM per modality (keeps same interface as original)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # handle [B, feat] -> [B,1,feat]\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]  # last timestep\n",
        "\n",
        "\n",
        "class DomainScalingMLP(nn.Module):\n",
        "    def __init__(self, input_dim=1, n_modalities=5, hidden=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, n_modalities),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, domain_disc):\n",
        "        return self.net(domain_disc)\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, n_modalities, hidden_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.key   = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.value = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.scaler = DomainScalingMLP(input_dim=1, n_modalities=n_modalities, hidden=16)\n",
        "\n",
        "    def forward(self, features, domain_disc, debug=False):\n",
        "        # features: list of [B, H] -> stack -> [B, M, H]\n",
        "        x = torch.stack(features, dim=1)  # [B, M, H]\n",
        "        Q, K, V = self.query(x), self.key(x), self.value(x)  # -> [B, M, F]\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Domain scaling (kept from original)\n",
        "        scaling = self.scaler(domain_disc).unsqueeze(1)  # [B,1,M]\n",
        "        attn = attn * scaling\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        if debug and DEBUG_ATTENTION:\n",
        "            print(\"Domain scaling factors:\", scaling[0, 0].detach().cpu().numpy())\n",
        "            print(\"Attention weights:\", attn[0].detach().cpu().numpy())\n",
        "\n",
        "        fused_per_query = torch.matmul(attn, V)  # [B, M, F]\n",
        "        fused = fused_per_query.mean(dim=1)      # [B, F] (average across queries)\n",
        "        return fused\n",
        "\n",
        "\n",
        "class DomainAdaptiveLayer(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.bn = nn.BatchNorm1d(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.fc(x))\n",
        "\n",
        "\n",
        "class FMAL_Daf(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=64, fusion_dim=128,\n",
        "                 num_sessions=3, use_time_and_session=True):\n",
        "        super().__init__()\n",
        "        self.use_time_and_session = use_time_and_session\n",
        "\n",
        "        # modality LSTMs\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(d, lstm_hidden) for d in modalities_dim])\n",
        "\n",
        "        # attention fusion\n",
        "        self.attn_fusion = CrossModalAttention(len(modalities_dim), lstm_hidden, fusion_dim)\n",
        "\n",
        "        # domain-adaptive transformation\n",
        "        self.domain_adapt = DomainAdaptiveLayer(fusion_dim)\n",
        "\n",
        "        # session embedding (instead of raw scalar)\n",
        "        self.session_emb = nn.Embedding(num_sessions, 8)\n",
        "\n",
        "        # elapsed time encoder (tiny MLP)\n",
        "        self.time_enc = nn.Sequential(\n",
        "            nn.Linear(1, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 8)\n",
        "        )\n",
        "\n",
        "        # global LSTM input = fusion_dim + 8(session) + 8(time)\n",
        "        global_input_dim = fusion_dim + 16 if use_time_and_session else fusion_dim\n",
        "        self.global_lstm = nn.LSTM(global_input_dim, 32, batch_first=True)\n",
        "\n",
        "        # regression heads\n",
        "        self.reg_phys = nn.Linear(32, 1)\n",
        "        self.reg_ment = nn.Linear(32, 1)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LSTM):\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, modal_inputs, domain_disc, session_feat=None, elapsed_feat=None, debug=False):\n",
        "        feats = [m(inp) for m, inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = self.attn_fusion(feats, domain_disc, debug=debug)\n",
        "        adapted = self.domain_adapt(fused)\n",
        "\n",
        "        if self.use_time_and_session:\n",
        "            if session_feat is None or elapsed_feat is None:\n",
        "                raise ValueError(\"Need session_feat and elapsed_feat\")\n",
        "\n",
        "            # session_feat is already int-mapped\n",
        "            session_emb = self.session_emb(session_feat.squeeze(-1).long())\n",
        "            time_emb = self.time_enc(elapsed_feat)  # normalized automatically\n",
        "            adapted = torch.cat([adapted, session_emb, time_emb], dim=1)\n",
        "\n",
        "        glstm_out, _ = self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_phys(feat), self.reg_ment(feat)\n",
        "\n",
        "\n",
        "# ========== Train / Eval ==========\n",
        "def train_model(model, loader, optim, crit_reg, epochs=10, device=torch.device(\"cpu\")):\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        tot = 0.0\n",
        "        for modals, dom, session_feat, elapsed_feat, (phys, ment) in loader:\n",
        "            modals = [m.to(device) for m in modals]\n",
        "            dom = dom.to(device)\n",
        "            session_feat = session_feat.to(device)\n",
        "            elapsed_feat = elapsed_feat.to(device)\n",
        "            phys = phys.to(device)\n",
        "            ment = ment.to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            p_pred, m_pred = model(modals, dom, session_feat, elapsed_feat)\n",
        "            loss = crit_reg(p_pred, phys) + 2*crit_reg(m_pred, ment)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            tot += loss.item()\n",
        "        avg_loss = tot / len(loader) if len(loader) > 0 else float('nan')\n",
        "        print(f\"[Epoch {ep}] Avg Training Loss: {avg_loss:.6f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model, test_loader, device, crit_reg):\n",
        "    model.eval()\n",
        "    regL = 0.0\n",
        "    all_phys_preds, all_phys_labels = [], []\n",
        "    all_ment_preds, all_ment_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for modals, dom, session_feat, elapsed_feat, (phys, ment) in test_loader:\n",
        "            modals = [m.to(device) for m in modals]\n",
        "            dom = dom.to(device)\n",
        "            session_feat = session_feat.to(device)\n",
        "            elapsed_feat = elapsed_feat.to(device)\n",
        "            phys = phys.to(device)\n",
        "            ment = ment.to(device)\n",
        "\n",
        "            p_pred, m_pred = model(modals, dom, session_feat, elapsed_feat)\n",
        "            loss_r = crit_reg(p_pred, phys) + crit_reg(m_pred, ment)\n",
        "            regL += loss_r.item()\n",
        "\n",
        "            all_phys_preds.extend(p_pred.squeeze().cpu().numpy())\n",
        "            all_phys_labels.extend(phys.squeeze().cpu().numpy())\n",
        "            all_ment_preds.extend(m_pred.squeeze().cpu().numpy())\n",
        "            all_ment_labels.extend(ment.squeeze().cpu().numpy())\n",
        "\n",
        "            # print a single batch sample comparison for debugging\n",
        "            print(\"=== Predictions vs Ground Truth (first batch) ===\")\n",
        "            for i in range(len(phys)):\n",
        "                print(f\"Sample {i}: True phys={phys[i].item():.3f}, True ment={ment[i].item():.3f} | \"\n",
        "                      f\"Pred phys={p_pred[i].item():.3f}, Pred ment={m_pred[i].item():.3f}\")\n",
        "            break\n",
        "\n",
        "    # Metrics\n",
        "    rmse_phys = math.sqrt(mean_squared_error(all_phys_labels, all_phys_preds))\n",
        "    mae_phys  = mean_absolute_error(all_phys_labels, all_phys_preds)\n",
        "    r2_phys   = r2_score(all_phys_labels, all_phys_preds)\n",
        "\n",
        "    rmse_ment = math.sqrt(mean_squared_error(all_ment_labels, all_ment_preds))\n",
        "    mae_ment  = mean_absolute_error(all_ment_labels, all_ment_preds)\n",
        "    r2_ment   = r2_score(all_ment_labels, all_ment_preds)\n",
        "\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss (sum): {regL/len(test_loader):.6f}\")\n",
        "    print(f\"Physical Fatigue - RMSE: {rmse_phys:.6f}, MAE: {mae_phys:.6f}, R²: {r2_phys:.6f}\")\n",
        "    print(f\"Mental Fatigue  - RMSE: {rmse_ment:.6f}, MAE: {mae_ment:.6f}, R²: {r2_ment:.6f}\")\n",
        "\n",
        "\n",
        "# ========== Main ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # replace with your CSV path\n",
        "    csv_path = '/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized_interpolated.csv'\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # create datasets (train/test split is left to you; here I do a simple random split for demo)\n",
        "    # IMPORTANT: do temporal or grouped splits in real experiments to avoid leakage\n",
        "    # Compute session_map from full dataset so train/test share mapping\n",
        "    session_map = {s: i for i, s in enumerate(sorted(df['session'].astype(int).unique()))}\n",
        "\n",
        "    # split indices: simple 80/20 random split but preserving rows (for demo)\n",
        "    shuffled = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    cut = int(0.8 * len(shuffled))\n",
        "    train_df = shuffled.iloc[:cut].reset_index(drop=True)\n",
        "    test_df  = shuffled.iloc[cut:].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = FatigueSessionDataset(train_df, session_map=session_map, compute_elapsed=True)\n",
        "    test_dataset  = FatigueSessionDataset(test_df,  session_map=session_map, compute_elapsed=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # modality dims are [HR(2), IBI(2), ACC(6), EDA(2), Temp(2)]\n",
        "    model = FMAL_Daf([2,2,6,2,2], lstm_hidden=32, fusion_dim=64, use_time_and_session=True).to(device)\n",
        "\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    crit_reg = nn.MSELoss()\n",
        "\n",
        "    train_model(model, train_loader, optim, crit_reg, epochs=47, device=device)\n",
        "    evaluate_and_show_predictions(model, test_loader, device, crit_reg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve5tDceWbQ14",
        "outputId": "89975d1b-e890-495b-fdb3-7a56a7167bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg Training Loss: 0.183955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg Training Loss: 0.146511\n",
            "[Epoch 3] Avg Training Loss: 0.140273\n",
            "[Epoch 4] Avg Training Loss: 0.136154\n",
            "[Epoch 5] Avg Training Loss: 0.138312\n",
            "[Epoch 6] Avg Training Loss: 0.128586\n",
            "[Epoch 7] Avg Training Loss: 0.124302\n",
            "[Epoch 8] Avg Training Loss: 0.121584\n",
            "[Epoch 9] Avg Training Loss: 0.124288\n",
            "[Epoch 10] Avg Training Loss: 0.116877\n",
            "[Epoch 11] Avg Training Loss: 0.112899\n",
            "[Epoch 12] Avg Training Loss: 0.112915\n",
            "[Epoch 13] Avg Training Loss: 0.110379\n",
            "[Epoch 14] Avg Training Loss: 0.107562\n",
            "[Epoch 15] Avg Training Loss: 0.114460\n",
            "[Epoch 16] Avg Training Loss: 0.111163\n",
            "[Epoch 17] Avg Training Loss: 0.111629\n",
            "[Epoch 18] Avg Training Loss: 0.101850\n",
            "[Epoch 19] Avg Training Loss: 0.100659\n",
            "[Epoch 20] Avg Training Loss: 0.104529\n",
            "[Epoch 21] Avg Training Loss: 0.103084\n",
            "[Epoch 22] Avg Training Loss: 0.099379\n",
            "[Epoch 23] Avg Training Loss: 0.099358\n",
            "[Epoch 24] Avg Training Loss: 0.099427\n",
            "[Epoch 25] Avg Training Loss: 0.097203\n",
            "[Epoch 26] Avg Training Loss: 0.095446\n",
            "[Epoch 27] Avg Training Loss: 0.092372\n",
            "[Epoch 28] Avg Training Loss: 0.092954\n",
            "[Epoch 29] Avg Training Loss: 0.092732\n",
            "[Epoch 30] Avg Training Loss: 0.091829\n",
            "[Epoch 31] Avg Training Loss: 0.090662\n",
            "[Epoch 32] Avg Training Loss: 0.093655\n",
            "[Epoch 33] Avg Training Loss: 0.086146\n",
            "[Epoch 34] Avg Training Loss: 0.090254\n",
            "[Epoch 35] Avg Training Loss: 0.083989\n",
            "[Epoch 36] Avg Training Loss: 0.082958\n",
            "[Epoch 37] Avg Training Loss: 0.084809\n",
            "[Epoch 38] Avg Training Loss: 0.079853\n",
            "[Epoch 39] Avg Training Loss: 0.082613\n",
            "[Epoch 40] Avg Training Loss: 0.074621\n",
            "[Epoch 41] Avg Training Loss: 0.080380\n",
            "[Epoch 42] Avg Training Loss: 0.081409\n",
            "[Epoch 43] Avg Training Loss: 0.079664\n",
            "[Epoch 44] Avg Training Loss: 0.077154\n",
            "[Epoch 45] Avg Training Loss: 0.078885\n",
            "[Epoch 46] Avg Training Loss: 0.076102\n",
            "[Epoch 47] Avg Training Loss: 0.072774\n",
            "=== Predictions vs Ground Truth (first batch) ===\n",
            "Sample 0: True phys=0.187, True ment=0.183 | Pred phys=0.107, Pred ment=0.135\n",
            "Sample 1: True phys=0.187, True ment=0.183 | Pred phys=0.033, Pred ment=0.042\n",
            "Sample 2: True phys=0.187, True ment=0.183 | Pred phys=-0.032, Pred ment=0.092\n",
            "Sample 3: True phys=0.188, True ment=0.183 | Pred phys=-0.010, Pred ment=0.122\n",
            "Sample 4: True phys=0.200, True ment=0.180 | Pred phys=0.135, Pred ment=0.152\n",
            "Sample 5: True phys=0.208, True ment=0.177 | Pred phys=0.108, Pred ment=0.113\n",
            "Sample 6: True phys=0.261, True ment=0.161 | Pred phys=0.325, Pred ment=0.340\n",
            "Sample 7: True phys=0.265, True ment=0.160 | Pred phys=0.335, Pred ment=0.336\n",
            "Sample 8: True phys=0.269, True ment=0.158 | Pred phys=0.349, Pred ment=0.319\n",
            "Sample 9: True phys=0.278, True ment=0.156 | Pred phys=0.326, Pred ment=0.347\n",
            "Sample 10: True phys=0.310, True ment=0.146 | Pred phys=0.283, Pred ment=0.377\n",
            "Sample 11: True phys=0.308, True ment=0.178 | Pred phys=0.324, Pred ment=0.259\n",
            "Sample 12: True phys=0.294, True ment=0.299 | Pred phys=0.284, Pred ment=0.402\n",
            "Sample 13: True phys=0.283, True ment=0.395 | Pred phys=0.289, Pred ment=0.394\n",
            "Sample 14: True phys=0.268, True ment=0.517 | Pred phys=0.288, Pred ment=0.392\n",
            "Sample 15: True phys=0.267, True ment=0.525 | Pred phys=0.286, Pred ment=0.391\n",
            "Sample 16: True phys=0.262, True ment=0.569 | Pred phys=0.279, Pred ment=0.391\n",
            "Sample 17: True phys=0.259, True ment=0.594 | Pred phys=0.277, Pred ment=0.389\n",
            "Sample 18: True phys=0.198, True ment=0.433 | Pred phys=0.410, Pred ment=0.240\n",
            "Sample 19: True phys=0.198, True ment=0.433 | Pred phys=0.234, Pred ment=0.078\n",
            "Sample 20: True phys=0.198, True ment=0.433 | Pred phys=0.293, Pred ment=0.204\n",
            "Sample 21: True phys=0.248, True ment=0.450 | Pred phys=0.323, Pred ment=0.188\n",
            "Sample 22: True phys=0.306, True ment=0.470 | Pred phys=0.306, Pred ment=0.176\n",
            "Sample 23: True phys=0.403, True ment=0.503 | Pred phys=0.310, Pred ment=0.291\n",
            "Sample 24: True phys=0.461, True ment=0.522 | Pred phys=0.373, Pred ment=0.427\n",
            "Sample 25: True phys=0.520, True ment=0.542 | Pred phys=0.532, Pred ment=0.484\n",
            "Sample 26: True phys=0.714, True ment=0.608 | Pred phys=0.660, Pred ment=0.555\n",
            "Sample 27: True phys=0.765, True ment=0.625 | Pred phys=0.579, Pred ment=0.498\n",
            "Sample 28: True phys=0.700, True ment=0.591 | Pred phys=0.507, Pred ment=0.411\n",
            "Sample 29: True phys=0.690, True ment=0.586 | Pred phys=0.508, Pred ment=0.456\n",
            "Sample 30: True phys=0.634, True ment=0.557 | Pred phys=0.446, Pred ment=0.395\n",
            "Sample 31: True phys=0.625, True ment=0.552 | Pred phys=0.436, Pred ment=0.385\n",
            "\n",
            "=== Final Test Performance ===\n",
            "Test Regression Loss (sum): 0.002259\n",
            "Physical Fatigue - RMSE: 0.112837, MAE: 0.087849, R²: 0.608215\n",
            "Mental Fatigue  - RMSE: 0.167097, MAE: 0.147390, R²: 0.104308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Centralized Model - Without Novelty\n"
      ],
      "metadata": {
        "id": "6XF1w4GSl0wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# =========================\n",
        "# Dataset (unchanged)\n",
        "# =========================\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    def __init__(self, df, session_map=None, compute_elapsed=True):\n",
        "        df = df.copy()\n",
        "        df['session'] = df['session'].astype(int)\n",
        "        df['person'] = df['person'].astype(int)\n",
        "        df['window_start'] = pd.to_datetime(df['window_start'])\n",
        "        if compute_elapsed:\n",
        "            df['elapsed_time'] = df.groupby(['person', 'session'])['window_start'] \\\n",
        "                                  .transform(lambda x: (x - x.min()).dt.total_seconds())\n",
        "        else:\n",
        "            df['elapsed_time'] = 0.0\n",
        "        self.data = df.sort_values(['person', 'session', 'window_start']).reset_index(drop=True)\n",
        "        if session_map is None:\n",
        "            all_sessions = sorted(self.data['session'].unique())\n",
        "            self.session_map = {s: i for i, s in enumerate(all_sessions)}\n",
        "        else:\n",
        "            self.session_map = session_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'], row['ax_std'], row['ay_mean'], row['ay_std'], row['az_mean'], row['az_std']], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        phys  = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment  = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "        session_index = self.session_map[int(row['session'])]\n",
        "        session_feat = torch.tensor([float(session_index)], dtype=torch.float32)\n",
        "        elapsed = torch.tensor([float(row['elapsed_time'])], dtype=torch.float32)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)  # placeholder\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, session_feat, elapsed, (phys, ment)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([b[0][i] for b in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([b[1] for b in batch])\n",
        "    session_feat = torch.stack([b[2] for b in batch])\n",
        "    elapsed_feat = torch.stack([b[3] for b in batch])\n",
        "    phys         = torch.stack([b[4][0] for b in batch])\n",
        "    ment         = torch.stack([b[4][1] for b in batch])\n",
        "    return modal_inputs, domain_disc, session_feat, elapsed_feat, (phys, ment)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Model\n",
        "# =========================\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "class BaselineFMAL(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=32, use_time_and_session=True, num_sessions=3):\n",
        "        super().__init__()\n",
        "        self.use_time_and_session = use_time_and_session\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(d, lstm_hidden) for d in modalities_dim])\n",
        "\n",
        "        # session embedding\n",
        "        self.session_emb = nn.Embedding(num_sessions, 8)\n",
        "\n",
        "        # elapsed time encoder\n",
        "        self.time_enc = nn.Sequential(\n",
        "            nn.Linear(1, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 8)\n",
        "        )\n",
        "\n",
        "        # global LSTM input = concat of modality LSTMs + session + time\n",
        "        global_input_dim = lstm_hidden * len(modalities_dim)\n",
        "        if use_time_and_session:\n",
        "            global_input_dim += 16\n",
        "        self.global_lstm = nn.LSTM(global_input_dim, 32, batch_first=True)\n",
        "\n",
        "        # regression heads\n",
        "        self.reg_phys = nn.Linear(32, 1)\n",
        "        self.reg_ment = nn.Linear(32, 1)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LSTM):\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, modal_inputs, domain_disc=None, session_feat=None, elapsed_feat=None, debug=False):\n",
        "        feats = [m(inp) for m, inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = torch.cat(feats, dim=1)\n",
        "\n",
        "        if self.use_time_and_session:\n",
        "            session_emb = self.session_emb(session_feat.squeeze(-1).long())\n",
        "            time_emb = self.time_enc(elapsed_feat)\n",
        "            fused = torch.cat([fused, session_emb, time_emb], dim=1)\n",
        "\n",
        "        glstm_out, _ = self.global_lstm(fused.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_phys(feat), self.reg_ment(feat)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Training / Evaluation (unchanged)\n",
        "# =========================\n",
        "def train_model(model, loader, optim, crit_reg, epochs=10, device=torch.device(\"cpu\")):\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        tot = 0.0\n",
        "        for modals, dom, session_feat, elapsed_feat, (phys, ment) in loader:\n",
        "            modals = [m.to(device) for m in modals]\n",
        "            session_feat = session_feat.to(device)\n",
        "            elapsed_feat = elapsed_feat.to(device)\n",
        "            phys = phys.to(device)\n",
        "            ment = ment.to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            p_pred, m_pred = model(modals, session_feat=session_feat, elapsed_feat=elapsed_feat)\n",
        "            loss = crit_reg(p_pred, phys) + 2*crit_reg(m_pred, ment)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            tot += loss.item()\n",
        "        avg_loss = tot / len(loader) if len(loader) > 0 else float('nan')\n",
        "        print(f\"[Epoch {ep}] Avg Training Loss: {avg_loss:.6f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model, test_loader, device, crit_reg):\n",
        "    model.eval()\n",
        "    regL = 0.0\n",
        "    all_phys_preds, all_phys_labels = [], []\n",
        "    all_ment_preds, all_ment_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for modals, dom, session_feat, elapsed_feat, (phys, ment) in test_loader:\n",
        "            modals = [m.to(device) for m in modals]\n",
        "            session_feat = session_feat.to(device)\n",
        "            elapsed_feat = elapsed_feat.to(device)\n",
        "            phys = phys.to(device)\n",
        "            ment = ment.to(device)\n",
        "\n",
        "            p_pred, m_pred = model(modals, session_feat=session_feat, elapsed_feat=elapsed_feat)\n",
        "            loss_r = crit_reg(p_pred, phys) + crit_reg(m_pred, ment)\n",
        "            regL += loss_r.item()\n",
        "\n",
        "            all_phys_preds.extend(p_pred.squeeze().cpu().numpy())\n",
        "            all_phys_labels.extend(phys.squeeze().cpu().numpy())\n",
        "            all_ment_preds.extend(m_pred.squeeze().cpu().numpy())\n",
        "            all_ment_labels.extend(ment.squeeze().cpu().numpy())\n",
        "\n",
        "            # print first batch only\n",
        "            print(\"=== Predictions vs Ground Truth (first batch) ===\")\n",
        "            for i in range(len(phys)):\n",
        "                print(f\"Sample {i}: True phys={phys[i].item():.3f}, True ment={ment[i].item():.3f} | \"\n",
        "                      f\"Pred phys={p_pred[i].item():.3f}, Pred ment={m_pred[i].item():.3f}\")\n",
        "            break\n",
        "\n",
        "    rmse_phys = math.sqrt(mean_squared_error(all_phys_labels, all_phys_preds))\n",
        "    mae_phys  = mean_absolute_error(all_phys_labels, all_phys_preds)\n",
        "    r2_phys   = r2_score(all_phys_labels, all_phys_preds)\n",
        "    rmse_ment = math.sqrt(mean_squared_error(all_ment_labels, all_ment_preds))\n",
        "    mae_ment  = mean_absolute_error(all_ment_labels, all_ment_preds)\n",
        "    r2_ment   = r2_score(all_ment_labels, all_ment_preds)\n",
        "\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss (sum): {regL/len(test_loader):.6f}\")\n",
        "    print(f\"Physical Fatigue - RMSE: {rmse_phys:.6f}, MAE: {mae_phys:.6f}, R²: {r2_phys:.6f}\")\n",
        "    print(f\"Mental Fatigue  - RMSE: {rmse_ment:.6f}, MAE: {mae_ment:.6f}, R²: {r2_ment:.6f}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    csv_path = '/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized_interpolated.csv'\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    session_map = {s: i for i, s in enumerate(sorted(df['session'].astype(int).unique()))}\n",
        "    shuffled = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    cut = int(0.8 * len(shuffled))\n",
        "    train_df = shuffled.iloc[:cut].reset_index(drop=True)\n",
        "    test_df  = shuffled.iloc[cut:].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = FatigueSessionDataset(train_df, session_map=session_map, compute_elapsed=True)\n",
        "    test_dataset  = FatigueSessionDataset(test_df,  session_map=session_map, compute_elapsed=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BaselineFMAL([2,2,6,2,2], lstm_hidden=32, use_time_and_session=True, num_sessions=len(session_map)).to(device)\n",
        "\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    crit_reg = nn.MSELoss()\n",
        "\n",
        "    train_model(model, train_loader, optim, crit_reg, epochs=47, device=device)\n",
        "    evaluate_and_show_predictions(model, test_loader, device, crit_reg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f02wciPul0g6",
        "outputId": "83f698a7-e49f-4897-b6b8-98a53e47c7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg Training Loss: 0.513347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg Training Loss: 0.161110\n",
            "[Epoch 3] Avg Training Loss: 0.156698\n",
            "[Epoch 4] Avg Training Loss: 0.155157\n",
            "[Epoch 5] Avg Training Loss: 0.154061\n",
            "[Epoch 6] Avg Training Loss: 0.152778\n",
            "[Epoch 7] Avg Training Loss: 0.154899\n",
            "[Epoch 8] Avg Training Loss: 0.154858\n",
            "[Epoch 9] Avg Training Loss: 0.151549\n",
            "[Epoch 10] Avg Training Loss: 0.149262\n",
            "[Epoch 11] Avg Training Loss: 0.150932\n",
            "[Epoch 12] Avg Training Loss: 0.150330\n",
            "[Epoch 13] Avg Training Loss: 0.152497\n",
            "[Epoch 14] Avg Training Loss: 0.148133\n",
            "[Epoch 15] Avg Training Loss: 0.149063\n",
            "[Epoch 16] Avg Training Loss: 0.146970\n",
            "[Epoch 17] Avg Training Loss: 0.147941\n",
            "[Epoch 18] Avg Training Loss: 0.148184\n",
            "[Epoch 19] Avg Training Loss: 0.146879\n",
            "[Epoch 20] Avg Training Loss: 0.146558\n",
            "[Epoch 21] Avg Training Loss: 0.145402\n",
            "[Epoch 22] Avg Training Loss: 0.145739\n",
            "[Epoch 23] Avg Training Loss: 0.142945\n",
            "[Epoch 24] Avg Training Loss: 0.144016\n",
            "[Epoch 25] Avg Training Loss: 0.143619\n",
            "[Epoch 26] Avg Training Loss: 0.141361\n",
            "[Epoch 27] Avg Training Loss: 0.140122\n",
            "[Epoch 28] Avg Training Loss: 0.142886\n",
            "[Epoch 29] Avg Training Loss: 0.141957\n",
            "[Epoch 30] Avg Training Loss: 0.139382\n",
            "[Epoch 31] Avg Training Loss: 0.138060\n",
            "[Epoch 32] Avg Training Loss: 0.139107\n",
            "[Epoch 33] Avg Training Loss: 0.137976\n",
            "[Epoch 34] Avg Training Loss: 0.136618\n",
            "[Epoch 35] Avg Training Loss: 0.135993\n",
            "[Epoch 36] Avg Training Loss: 0.136891\n",
            "[Epoch 37] Avg Training Loss: 0.135483\n",
            "[Epoch 38] Avg Training Loss: 0.137769\n",
            "[Epoch 39] Avg Training Loss: 0.132292\n",
            "[Epoch 40] Avg Training Loss: 0.132129\n",
            "[Epoch 41] Avg Training Loss: 0.132714\n",
            "[Epoch 42] Avg Training Loss: 0.130349\n",
            "[Epoch 43] Avg Training Loss: 0.131351\n",
            "[Epoch 44] Avg Training Loss: 0.128653\n",
            "[Epoch 45] Avg Training Loss: 0.131946\n",
            "[Epoch 46] Avg Training Loss: 0.129705\n",
            "[Epoch 47] Avg Training Loss: 0.128395\n",
            "=== Predictions vs Ground Truth (first batch) ===\n",
            "Sample 0: True phys=0.187, True ment=0.183 | Pred phys=0.221, Pred ment=0.117\n",
            "Sample 1: True phys=0.187, True ment=0.183 | Pred phys=0.213, Pred ment=0.315\n",
            "Sample 2: True phys=0.187, True ment=0.183 | Pred phys=0.124, Pred ment=0.228\n",
            "Sample 3: True phys=0.188, True ment=0.183 | Pred phys=0.152, Pred ment=0.326\n",
            "Sample 4: True phys=0.200, True ment=0.180 | Pred phys=0.161, Pred ment=0.321\n",
            "Sample 5: True phys=0.208, True ment=0.177 | Pred phys=0.158, Pred ment=0.291\n",
            "Sample 6: True phys=0.261, True ment=0.161 | Pred phys=0.199, Pred ment=0.340\n",
            "Sample 7: True phys=0.265, True ment=0.160 | Pred phys=0.202, Pred ment=0.345\n",
            "Sample 8: True phys=0.269, True ment=0.158 | Pred phys=0.204, Pred ment=0.343\n",
            "Sample 9: True phys=0.278, True ment=0.156 | Pred phys=0.206, Pred ment=0.349\n",
            "Sample 10: True phys=0.310, True ment=0.146 | Pred phys=0.198, Pred ment=0.344\n",
            "Sample 11: True phys=0.308, True ment=0.178 | Pred phys=0.202, Pred ment=0.353\n",
            "Sample 12: True phys=0.294, True ment=0.299 | Pred phys=0.219, Pred ment=0.379\n",
            "Sample 13: True phys=0.283, True ment=0.395 | Pred phys=0.245, Pred ment=0.403\n",
            "Sample 14: True phys=0.268, True ment=0.517 | Pred phys=0.281, Pred ment=0.433\n",
            "Sample 15: True phys=0.267, True ment=0.525 | Pred phys=0.280, Pred ment=0.433\n",
            "Sample 16: True phys=0.262, True ment=0.569 | Pred phys=0.285, Pred ment=0.436\n",
            "Sample 17: True phys=0.259, True ment=0.594 | Pred phys=0.299, Pred ment=0.448\n",
            "Sample 18: True phys=0.198, True ment=0.433 | Pred phys=0.223, Pred ment=0.153\n",
            "Sample 19: True phys=0.198, True ment=0.433 | Pred phys=0.129, Pred ment=0.214\n",
            "Sample 20: True phys=0.198, True ment=0.433 | Pred phys=0.116, Pred ment=0.236\n",
            "Sample 21: True phys=0.248, True ment=0.450 | Pred phys=0.132, Pred ment=0.245\n",
            "Sample 22: True phys=0.306, True ment=0.470 | Pred phys=0.149, Pred ment=0.256\n",
            "Sample 23: True phys=0.403, True ment=0.503 | Pred phys=0.167, Pred ment=0.269\n",
            "Sample 24: True phys=0.461, True ment=0.522 | Pred phys=0.194, Pred ment=0.277\n",
            "Sample 25: True phys=0.520, True ment=0.542 | Pred phys=0.280, Pred ment=0.360\n",
            "Sample 26: True phys=0.714, True ment=0.608 | Pred phys=0.440, Pred ment=0.442\n",
            "Sample 27: True phys=0.765, True ment=0.625 | Pred phys=0.411, Pred ment=0.459\n",
            "Sample 28: True phys=0.700, True ment=0.591 | Pred phys=0.314, Pred ment=0.400\n",
            "Sample 29: True phys=0.690, True ment=0.586 | Pred phys=0.322, Pred ment=0.431\n",
            "Sample 30: True phys=0.634, True ment=0.557 | Pred phys=0.293, Pred ment=0.418\n",
            "Sample 31: True phys=0.625, True ment=0.552 | Pred phys=0.289, Pred ment=0.416\n",
            "\n",
            "=== Final Test Performance ===\n",
            "Test Regression Loss (sum): 0.003317\n",
            "Physical Fatigue - RMSE: 0.177671, MAE: 0.130585, R²: 0.028643\n",
            "Mental Fatigue  - RMSE: 0.167750, MAE: 0.157128, R²: 0.097301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing Data heads"
      ],
      "metadata": {
        "id": "kH11iFAQzxKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔎 Show dataset splits\n",
        "print(\"\\n=== Training Dataset Head ===\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\n=== Testing Dataset Head ===\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "id": "Ss9JACVCb-XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}